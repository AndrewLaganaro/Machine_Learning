{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import linalg\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ***Linear Discriminant Analsysis (LDA)*** é uma técnica muito utilizada para redução de dimensionalidade na etapa de pré-processamento em aplicações de reconhecimento de padrões e aprendizagem de máquina. **O objetivo é projetar o dataset em um espaço de menor dimensões com uma boa separação das classes para evitar overfitting e reduzir o custo computacional**.\n",
    "\n",
    "A abordagem do LDA é bem semelhante a do PCA, no entanto, **além de encontrar os componentes que maximizam a variância dos nossos dados (PCA), nós também estamos interessados nos componentes que maximizam a separação entre múltiplas classes (LDA)**.\n",
    "\n",
    "Resumindo, o objetivo do LDA é projetar um espaço de atributos (um banco de dados com amostras n-dimensional) em um subespaço menor ***k*** (onde $k \\leq n-1$) mantendo a informação de discriminação das classes.\n",
    "\n",
    "Em geral, a redução de dimensionalidade ajuda não somente a reduzir custos computacionais para um dado problema de classificação, mas também é útil para evitar o overfitting pela minimização do erro na estimação de parâmetros.\n",
    "\n",
    "## PCA vs LDA\n",
    "\n",
    "Ambos PCA e LDA são técnicas de transformações lineares bastante utilizadas para redução de dimensionalidade. Por um lado, o PCA pode ser descrito como um algoritmo \"não-supervisionado\", já que ele \"ignora\" os rótulos das classes e seu objetivo é encontrar as direções (componentes principais) que maximizam a variância no banco de dados. Por outro lado, o LDA é \"supervisionado\" e calcula as direções (discriminantes lineares) que vão representar os eixos que maximizam a separação entre múltiplas classes.\n",
    "\n",
    "Embora pareça que o LDA seja superior ao PCA em problemas de multi-classificação onde os rótulos das classes são conhecidos, nem sempre isso acontece. Por exemplo, comparações entre acurácias de classificação para reconhecimento de imagens após o uso de PCA ou LDA mostra que **o PCA tende a ser melhor que o LDA se o número de amostras/classe é relativamente pequeno** (<a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=908974\">PCA vs LDA</a>, A.M. Martinez et al., 2001). Na prática, também é comum usar ambos LDA e PCA em conjunto, isto é, PCA para redução de dimensionalidade seguido por um LDA.\n",
    "\n",
    "<img src=\"images/PCAvsLDA.png\" width=600>\n",
    "\n",
    "### O que é um \"bom\" subespaço?\n",
    "\n",
    "Supondo que nosso objetivo é reduzir as dimensões de um dataset ***d***-dimensional pela projeção em um subespaço ***k***-dimensional (onde ***k < d***). Então, como saber qual tamanho devemos escolher para ***k*** (***k*** = o número de dimensões do novo subespaço de atributos), e como saber se nós temos um espaço de atributos que representa \"bem\" nossos dados?\n",
    "\n",
    "Em breve, nós vamos calcular os autovetores (componentes) do nosso dataset e coletá-los em matrizes chamadas *matrizes-esparsas (scatter-matrizes)*, ou melhor, matrizes esparsas intra-classes e inter-classes. Cada um desses autovetores é associado a um autovalor que nos diz o \"tamanho\" ou \"magnitude\" dos autovetores.\n",
    "\n",
    "**Se observamos que todos autovalores tem uma magnitude similar, então isso pode ser um bom indicador que nossos dados já estão projetados em um \"bom\" espaço de atributos.**\n",
    "\n",
    "Por outro lado, se alguns autovalores tem a magnitude muito maior que a dos outros, devemos escolher seus autovetores já que eles contém mais informação sobre a distribuição dos nossos dados. Da mesma forma, autovalores próximos a zero são menos informativos e devemos desconsiderá-los na construção do nosso subespaço.\n",
    "\n",
    "## Aplicação\n",
    "\n",
    "Em geral, a aplicação do LDA envolve a aplicação dos seguintes passos:\n",
    "1. Calcular a média (vetor **d**-dimensional) para cada uma das classes do dataset\n",
    "2. Calcular as scatter-matrices (intra-classe e inter-classe)\n",
    "3. Calcular os autovetores ($e_1, e_2, ..., e_d$) e seus correspondentes autovalores ($\\lambda_1, \\lambda_2,...\\lambda_d$) para as scatter-matrices.\n",
    "4. Ordenar os autovetores pelos autovalores em ordem decrescente e escolher os ***k*** autovetores com os maiores autovalores para formar uma matriz **W** [$d \\times k$], onde cada coluna representa um autovetor.\n",
    "5. Usar **W** para transformar as amostras no novo subespaço. Isso pode ser resumido pela multiplicação de matrizes: $Y = X \\times W$ (onde **X** [$n \\times d$] é a matriz que representa nosso dataset com ***n*** amostras, e **Y** é matriz das amostras transformadas no novo subespaço [$n \\times k$]).\n",
    "\n",
    "## Suposições de Normalidade\n",
    "\n",
    "Assim como outros algoritmos, o LDA assume que os dados são normalmente distribuidos, os atributos são estatisticamente independentes e matriz de covariância idênticas para cada classe. Entretanto, isso só se aplica ao LDA como classificador. Como redutor de dimensionalidade, o LDA também funciona razoavelmente bem se essas suposições forem violadas. E mesmo para tarefas de classificação o LDA pode ser robusto a distribuição dos dados:\n",
    "\n",
    "> “linear discriminant analysis frequently achieves good performances in the tasks of face and object recognition, even though the assumptions of common covariance matrix among groups and normality are often violated (Duda, et al., 2001)” <a href=\"http://link.springer.com/article/10.1007%2Fs10115-006-0013-y\">(Tao Li, et al., 2006)</a>.\n",
    "\n",
    "O escalamento de atributos (como padronização) **não** afeta os resultados gerais de um LDA e, portanto, pode ser opcional. Obviamente, as matrizes scatter vão ser diferentes dependendo se os atributos foram normalizados ou não. Além disso, os autovetores vão ser diferentes também. Entretanto, a parte importante é que os autovalores vão ser exatamente os mesmos como também as projeções finais - a única diferença que você notará é a escala dos eixos. Isso pode ser mostrado matematicamente (trabalhos futuros) e você pode encontrar nas referências a demonstração prática.\n",
    "\n",
    "## Algoritmo\n",
    "\n",
    "### Passo 1: Calcular os médios d-dimensionais\n",
    "\n",
    "Vamos começar calculando os vetores médios **$\\mu$** de cada atributos para todas as amostras de cada classe $C$ presente nos nossos dados. Logo, o vetor $\\mu$ terá $C$ linhas (#classes) e $F$ linhas (#atributos).\n",
    "\n",
    "### Passo 2: Calcular as Scatter Matrices\n",
    "\n",
    "A matriz scatter **intra-classe** $S_W$ é calculada pela seguinte equação:\n",
    "\n",
    "$$S_W = \\sum_{c=1}^{C} \\frac{1}{N_c} (x_c - \\mu_c)^T(x_c - \\mu_c)$$\n",
    "\n",
    "Onde $N_c$ é a #amostra de uma determinada classe $c$\n",
    "\n",
    "Repare que $(x_c - \\mu_c)^T(x_c - \\mu_c)$ representa o cálculo da **matriz de covariância**. Repare também que descartar o termo $\\frac{1}{N_c}$, apenas deixa matriz resultante descalonada. **Entretanto, o autoespaçoes resultantes seriam os mesmos (mesmo autovetores, somente os autovalores seriam diferentemente escalados por um fator constante)**.\n",
    "\n",
    "A scatter matrix **entre-classes** $S_B$ é calculada pela seguinte equação:\n",
    "\n",
    "$$S_B =  \\sum_{c=1}^{c} \\frac{1}{N_c} (\\mu_c - \\bar x)^T(\\mu_c - \\bar x)$$\n",
    "\n",
    "Onde $\\bar x$ representa a média de todas as amostras por atributo.\n",
    "\n",
    "Uma vez que $S_W$ representa a matriz *intra-classe* e a covariância total (de todas as amostras) é dada por $S_T$, você também pode calcular $S_B = S_T - S_W$. \n",
    "\n",
    "### Passo 3: Cálculo dos autovalores para a matriz $S_W^{-1} S_B$\n",
    "\n",
    "Os autovalores e o autovetores são calculados a partir da seguinte fórmula:\n",
    "\n",
    "$$S_W^{-1} S_B$$\n",
    "\n",
    "A título de recordação, ambos autovetores e autovalores nos dão informações a respeito da distorção em uma transformação linear: os **autovetores são basicamente a direção dessa distorção**, enquanto os **autovalores representam o fator de escala para os autovetores que descrevem a magnitude da distorção**.\n",
    "\n",
    "Se estamos aplicando o LDA para redução de dimensionalidade, os autovetores são importantes uma vez que eles vão formar os novos eixos do novo subespaço de atributos; os autovalores associados são particularmente importantes pois nos dizem quão \"informativos\" os novos eixos são.\n",
    "\n",
    "#### Verificação dos autovetores e autovalores\n",
    "\n",
    "Podemos verificar os nosso autovetores e autovalores calculados pela verificação da seguinte equação:\n",
    "\n",
    "$$Av = \\lambda v$$\n",
    "\n",
    "onde,\n",
    "$$A = S_W^{-1}S_B$$\n",
    "$$v = autovetor$$\n",
    "$$\\lambda = autovalor$$\n",
    "\n",
    "### Passo 4: Selecão dos discriminantes lineares\n",
    "\n",
    "Não estamos interessado em somente projetar os dados em um subespaço que aumenta a separação entre as classes, mas também reduzir a dimensionalidade do nosso espaço de atributos. Entretanto, os autovetores somente definem as direções dos novos eixos, já que todos são vetores unitários.\n",
    "\n",
    "Logo, para decidir quai(s) autovetor(es) nós escolheremos, iremos olhar os autovalores correspondentes de cada autovetor. De maneira geral, vamos mantes os autovetores com os maiores autovalores associados a eles.\n",
    "\n",
    "A maneira mais comum de fazer isso é ordenar os autovalores em ordem decrescente e escolher os top-k autovetores associados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse tutorial, vamos utilizar o **Iris dataset**, já presente no scikit-learn. O Iris dataset contém dados de 150 flores divididas em 3 espécies diferentes (setosa, versicolor, virginica). Os dados são:\n",
    "\n",
    "1. sepal lenght em cm\n",
    "2. sepal width em cm\n",
    "3. petal length em cm\n",
    "4. petal width em cm\n",
    "\n",
    "<img src=\"images/Iris-dataset.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     class  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "145      2  \n",
       "146      2  \n",
       "147      2  \n",
       "148      2  \n",
       "149      2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['class'] = iris.target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)       class  \n",
       "count        150.000000  150.000000  \n",
       "mean           1.199333    1.000000  \n",
       "std            0.762238    0.819232  \n",
       "min            0.100000    0.000000  \n",
       "25%            0.300000    0.000000  \n",
       "50%            1.300000    1.000000  \n",
       "75%            1.800000    2.000000  \n",
       "max            2.500000    2.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "x = df.drop(labels='class', axis=1).values\n",
    "y = df['class'].values\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.priors_ = None\n",
    "        self.means_ = []\n",
    "        self.covariance_ = []\n",
    "        self.overall_mean = 0.0\n",
    "        self.eigen_values = None\n",
    "        self.eigen_vectors = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        classes = np.unique(y)\n",
    "        n_classes = len(classes)\n",
    "        _, n_features = x.shape\n",
    "        self.priors_ = np.bincount(y) / float(len(y))\n",
    "        self.max_components = n_classes - 1 if self.n_components is None else np.clip(self.n_components, 1, n_classes - 1)\n",
    "\n",
    "        # Passo 1: calcular os vetores médios para cada classe\n",
    "        self.means_ = np.array([np.mean(x[y == c], axis=0) for c in classes])\n",
    "\n",
    "        # Passo 2: calcular as scatter matrices\n",
    "        overall_mean = np.mean(x, axis=0).reshape(1, n_features)\n",
    "        self.covariance_ = np.zeros(shape=(n_features, n_features))\n",
    "        S_w = np.zeros(shape=(n_features, n_features))\n",
    "        S_b = np.zeros(shape=(n_features, n_features))\n",
    "        for p, c, mean_vec in zip(self.priors_, classes, self.means_):\n",
    "            class_samples = x[y == c]\n",
    "\n",
    "            self.covariance_ += (p * np.cov(class_samples, rowvar=False, bias=1))\n",
    "            S_w += p * np.dot((class_samples - mean_vec).T, (class_samples - mean_vec))\n",
    "            S_b += p * np.dot((mean_vec - overall_mean).T, mean_vec - overall_mean)\n",
    "\n",
    "        # Passo 3: calcular os autovalores e autovetores\n",
    "        self.eigen_values, self.eigen_vectors = np.linalg.eig(np.linalg.inv(S_w).dot(S_b))\n",
    "        self.eigen_vectors *= -1\n",
    "\n",
    "        # Passo 4: seleção dos discriminantes lineares\n",
    "        self.sorted_components_ = np.argsort(self.eigen_values)[::-1]\n",
    "\n",
    "        self.projection_matrix_ = self.eigen_vectors[:, self.sorted_components_[:self.max_components]]\n",
    "\n",
    "        self.explained_variance_ = self.eigen_values[self.sorted_components_]\n",
    "        self.explained_variance_ratio_ = self.explained_variance_ / self.eigen_values.sum()\n",
    "\n",
    "    def predict(self, x):\n",
    "        cov_inv = np.linalg.inv(self.covariance_)\n",
    "\n",
    "        discriminants = []\n",
    "        for p, mean_vec in zip(self.priors_, self.means_):\n",
    "            first_term = np.dot(x, cov_inv).dot(mean_vec.T).reshape(-1, 1)\n",
    "            second_term = 0.5 * (np.dot(mean_vec, cov_inv).dot(mean_vec.T))\n",
    "            discriminants.append(first_term - second_term + np.log(p))\n",
    "\n",
    "        discriminants = np.hstack(discriminants)\n",
    "        return np.argmax(discriminants, axis=1)\n",
    "\n",
    "    def transform(self, x):\n",
    "        return np.dot(x, self.projection_matrix_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Teste "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.006 3.428 1.462 0.246]\n",
      " [5.936 2.77  4.26  1.326]\n",
      " [6.588 2.974 5.552 2.026]]\n",
      "[[0.259708   0.09086667 0.164164   0.03763333]\n",
      " [0.09086667 0.11308    0.05413867 0.032056  ]\n",
      " [0.164164   0.05413867 0.181484   0.041812  ]\n",
      " [0.03763333 0.032056   0.041812   0.041044  ]]\n",
      "[[-0.20874182  0.00653196 -0.69144732  0.2221981 ]\n",
      " [-0.38620369  0.58661055  0.01729421 -0.39892126]\n",
      " [ 0.55401172 -0.25256154 -0.04435297 -0.46129787]\n",
      " [ 0.7073504   0.76945309  0.72085667  0.76071947]]\n",
      "[ 9.91212605e-01  8.78739503e-03  1.17070280e-16 -2.14429714e-17]\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "lda.fit(x, y)\n",
    "\n",
    "print(lda.means_)\n",
    "print(lda.covariance_)\n",
    "print(lda.eigen_vectors)\n",
    "print(lda.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.49920971  1.88675441]\n",
      " [-1.2643595   1.59214275]\n",
      " [-1.35525305  1.73341462]\n",
      " [-1.18495616  1.62358806]\n",
      " [-1.5169559   1.94476227]]\n",
      "[0 0 0 0 1 1 1 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "x_proj = lda.transform(x)\n",
    "y_pred = lda.predict(x[::15])\n",
    "\n",
    "print(x_proj[:5])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação com o Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.006 3.428 1.462 0.246]\n",
      " [5.936 2.77  4.26  1.326]\n",
      " [6.588 2.974 5.552 2.026]]\n",
      "[[0.259708   0.09086667 0.164164   0.03763333]\n",
      " [0.09086667 0.11308    0.05413867 0.032056  ]\n",
      " [0.164164   0.05413867 0.181484   0.041812  ]\n",
      " [0.03763333 0.032056   0.041812   0.041044  ]]\n",
      "[[ 0.20874182  0.00653196 -0.52465543  0.44550654]\n",
      " [ 0.38620369  0.58661055 -0.09851721 -0.43741869]\n",
      " [-0.55401172 -0.25256154 -0.1686191  -0.48676231]\n",
      " [-0.7073504   0.76945309  0.82861248  0.61094294]]\n",
      "[0.9912126 0.0087874]\n"
     ]
    }
   ],
   "source": [
    "lda_sk = LinearDiscriminantAnalysis(solver='eigen', store_covariance=True)\n",
    "lda_sk.fit(x, y)\n",
    "\n",
    "print(lda_sk.means_)\n",
    "print(lda_sk.covariance_)\n",
    "print(lda_sk.scalings_)\n",
    "print(lda_sk.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.49920971 1.88675441]\n",
      " [1.2643595  1.59214275]\n",
      " [1.35525305 1.73341462]\n",
      " [1.18495616 1.62358806]\n",
      " [1.5169559  1.94476227]]\n",
      "[0 0 0 0 1 1 1 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "x_proj_sk = lda_sk.transform(x)\n",
    "y_pred_sk = lda_sk.predict(x[::15])\n",
    "\n",
    "print(x_proj_sk[:5])\n",
    "print(y_pred_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Antigo notebook do LDA com explicação passo-a-passo](https://github.com/arnaldog12/Machine_Learning/blob/a0d4999fc7efc51ce6a44acac9282e8dc512fa45/LDA.ipynb)\n",
    "- [Implementação Original do Scikit-learn](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/discriminant_analysis.py)\n",
    "- [Linear Discriminant Analysis](https://sebastianraschka.com/Articles/2014_python_lda.html)\n",
    "- [Linear discriminant analysis: A detailed tutorial](https://www.researchgate.net/publication/316994943_Linear_discriminant_analysis_A_detailed_tutorial)\n",
    "- [Classification — Linear Discriminant Analysis](https://towardsdatascience.com/classification-part-2-linear-discriminant-analysis-ea60c45b9ee5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
