{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados de cada tipo de problema (regressão, classificação binária e multiclasse) estão definidos nos testes da implementação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementação "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Funções de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, derivative=False):\n",
    "    return np.ones_like(x) if derivative else x\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = sigmoid(x)\n",
    "        return y*(1 - y)\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = tanh(x)\n",
    "        return 1 - y**2\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, derivative=False):\n",
    "    alpha = 0.1\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, alpha, 1)\n",
    "    return np.where(x <= 0, alpha*x, x)\n",
    "\n",
    "def elu(x, derivative=False):\n",
    "    alpha = 1.0\n",
    "    if derivative:\n",
    "        y = elu(x)\n",
    "        return np.where(x <= 0, y + alpha, 1)\n",
    "    return np.where(x <= 0, alpha*(np.exp(x) - 1), x)\n",
    "\n",
    "# other functions\n",
    "def softmax(x, y_oh=None, derivative=False):\n",
    "    if derivative: \n",
    "        y_pred = softmax(x)\n",
    "        y_correct = np.argmax(y_oh, axis=1)\n",
    "        pk = y_pred[range(y_pred.shape[0]), y_correct]\n",
    "        y_pred[range(y_pred.shape[0]), y_correct] = pk*(1.0 - pk)\n",
    "        return y_pred\n",
    "    exp = np.exp(x)\n",
    "    return exp/np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "def neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
    "    y_correct = np.argmax(y_oh, axis=1)\n",
    "    pk = y_pred[range(y_pred.shape[0]), y_correct]\n",
    "    if derivative:\n",
    "        y_pred[range(y_pred.shape[0]), y_correct] = (-1.0/pk)\n",
    "        return y_pred\n",
    "    return np.mean(-np.log(pk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Custo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost functions\n",
    "def mae(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(y_pred > y, 1, -1) / y.shape[0]\n",
    "    return np.mean(np.abs(y - y_pred))\n",
    "\n",
    "def mse(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred) / y.shape[0]\n",
    "    return 0.5*np.mean((y - y_pred)**2)\n",
    "\n",
    "def binary_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred) / (y_pred * (1-y_pred) * y.shape[0])\n",
    "    return -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "\n",
    "def softmax_neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
    "    y_softmax = softmax(y_pred)\n",
    "    y_correct = np.argmax(y_oh, axis=1)\n",
    "    pk = y_softmax[range(y_softmax.shape[0]), y_correct]\n",
    "    if derivative:\n",
    "        return -(y_oh - y_softmax)/y_oh.shape[0]\n",
    "    return np.mean(-np.log(pk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checagem dos Gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __compute_approx_grads(nn, x, y, eps=1e-4):\n",
    "    approx_grads = []\n",
    "    feed_forward = lambda inp: nn._NeuralNetwork__feedforward(inp)\n",
    "\n",
    "    for layer in nn.layers:\n",
    "        w_ori = layer.weights.copy()\n",
    "        w_ravel = w_ori.ravel()\n",
    "        w_shape = w_ori.shape\n",
    "\n",
    "        for i in range(w_ravel.size):\n",
    "            w_plus = w_ravel.copy()\n",
    "            w_plus[i] += eps\n",
    "            layer.weights = w_plus.reshape(w_shape)\n",
    "            J_plus = nn.cost_func(y, feed_forward(x))\n",
    "\n",
    "            w_minus = w_ravel.copy()\n",
    "            w_minus[i] -= eps\n",
    "            layer.weights = w_minus.reshape(w_shape)\n",
    "            J_minus = nn.cost_func(y, feed_forward(x))\n",
    "            approx_grads.append((J_plus - J_minus) / (2.0*eps))\n",
    "        layer.weights = w_ori\n",
    "\n",
    "    return approx_grads\n",
    "\n",
    "def gradient_checking(nn, x, y, eps=1e-4, verbose=False, verbose_precision=5):\n",
    "    from copy import deepcopy\n",
    "    nn_copy = deepcopy(nn)\n",
    "\n",
    "    nn.fit(x, y, epochs=0)\n",
    "    grads = np.concatenate([layer._dweights.ravel() for layer in nn.layers])\n",
    "\n",
    "    approx_grads = __compute_approx_grads(nn_copy, x, y, eps)\n",
    "\n",
    "    is_close = np.allclose(grads, approx_grads)\n",
    "    print(\"{}\".format(\"\\033[92mGRADIENTS OK\" if is_close else \"\\033[91mGRADIENTS FAIL\"))\n",
    "\n",
    "    norm_num = np.linalg.norm(grads - approx_grads)\n",
    "    norm_den = np.linalg.norm(grads) + np.linalg.norm(approx_grads)\n",
    "    error = norm_num / norm_den\n",
    "    print(\"Relative error:\", error)\n",
    "\n",
    "    if verbose:\n",
    "        np.set_printoptions(precision=verbose_precision, linewidth=200, suppress=True)\n",
    "        print(\"Gradientes:\", grads)\n",
    "        print(\"Aproximado:\", np.array(approx_grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, input_dim, output_dim, activation=linear):\n",
    "        self.input = None\n",
    "        self.weights = np.random.randn(output_dim, input_dim)\n",
    "        self.biases = np.random.randn(1, output_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "        self._activ_inp, self._activ_out = None, None\n",
    "        self._dweights, self._dbiases = None, None\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, cost_func=mse, learning_rate=1e-3):\n",
    "        self.layers = []\n",
    "        self.cost_func = cost_func\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self, x_train, y_train, epochs=100, verbose=10):\n",
    "        for epoch in range(epochs+1):\n",
    "            y_pred = self.__feedforward(x_train)\n",
    "            self.__backprop(y_train, y_pred)\n",
    "            \n",
    "            if epoch % verbose == 0:\n",
    "                loss_train = self.cost_func(y_train, self.predict(x_train))\n",
    "                print(\"epoch: {0:=4}/{1} loss_train: {2:.8f}\".format(epoch, epochs, loss_train))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.__feedforward(x)\n",
    "    \n",
    "    def __feedforward(self, x):\n",
    "        self.layers[0].input = x\n",
    "        for current_layer, next_layer in zip(self.layers, self.layers[1:] + [Layer(0, 0)]):\n",
    "            y = np.dot(current_layer.input, current_layer.weights.T) + current_layer.biases\n",
    "            current_layer._activ_inp = y\n",
    "            current_layer._activ_out = next_layer.input = current_layer.activation(y)\n",
    "        return self.layers[-1]._activ_out\n",
    "    \n",
    "    def __backprop(self, y, y_pred):\n",
    "        last_delta = self.cost_func(y, y_pred, derivative=True)\n",
    "        for layer in reversed(self.layers):\n",
    "            dactivation = layer.activation(layer._activ_inp, derivative=True) * last_delta\n",
    "            last_delta = np.dot(dactivation, layer.weights)\n",
    "            layer._dweights = np.dot(dactivation.T, layer.input)\n",
    "            layer._dbiases = 1.0*dactivation.sum(axis=0, keepdims=True)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.weights = layer.weights - self.learning_rate*layer._dweights\n",
    "            layer.biases = layer.biases - self.learning_rate*layer._dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Teste "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "N, D = 100, 2\n",
    "x = np.random.rand(N, D)\n",
    "y = np.random.rand(N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/100 loss_train: 0.09443909\n",
      "epoch:   10/100 loss_train: 0.08027429\n",
      "epoch:   20/100 loss_train: 0.06997904\n",
      "epoch:   30/100 loss_train: 0.06244557\n",
      "epoch:   40/100 loss_train: 0.05690320\n",
      "epoch:   50/100 loss_train: 0.05280786\n",
      "epoch:   60/100 loss_train: 0.04977095\n",
      "epoch:   70/100 loss_train: 0.04751232\n",
      "epoch:   80/100 loss_train: 0.04582842\n",
      "epoch:   90/100 loss_train: 0.04457045\n",
      "epoch:  100/100 loss_train: 0.04362907\n",
      "epoch:    0/0 loss_train: 0.04354904\n",
      "\u001b[92mGRADIENTS OK\n",
      "Relative error: 9.639094044703491e-08\n"
     ]
    }
   ],
   "source": [
    "D_in, D_out = x.shape[1], y.shape[1]\n",
    "nn = NeuralNetwork(cost_func=mse, learning_rate=1e-3)\n",
    "nn.layers.append(Layer(input_dim=D_in, output_dim=4, activation=relu))\n",
    "nn.layers.append(Layer(input_dim=4, output_dim=1, activation=tanh))\n",
    "nn.layers.append(Layer(input_dim=1, output_dim=2, activation=sigmoid))\n",
    "nn.layers.append(Layer(input_dim=2, output_dim=5, activation=leaky_relu))\n",
    "nn.layers.append(Layer(input_dim=5, output_dim=3, activation=elu))\n",
    "nn.layers.append(Layer(input_dim=3, output_dim=D_out, activation=linear))\n",
    "\n",
    "nn.fit(x, y, epochs=100)\n",
    "gradient_checking(nn, x, y, eps=1e-4, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Classificação Binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/100 loss_train: 0.89516990\n",
      "epoch:   10/100 loss_train: 0.88638120\n",
      "epoch:   20/100 loss_train: 0.87791389\n",
      "epoch:   30/100 loss_train: 0.86975853\n",
      "epoch:   40/100 loss_train: 0.86190582\n",
      "epoch:   50/100 loss_train: 0.85434657\n",
      "epoch:   60/100 loss_train: 0.84707176\n",
      "epoch:   70/100 loss_train: 0.84007248\n",
      "epoch:   80/100 loss_train: 0.83334002\n",
      "epoch:   90/100 loss_train: 0.82686579\n",
      "epoch:  100/100 loss_train: 0.82064138\n",
      "epoch:    0/0 loss_train: 0.82003236\n",
      "\u001b[92mGRADIENTS OK\n",
      "Relative error: 8.649640788422605e-10\n"
     ]
    }
   ],
   "source": [
    "y = np.random.randint(0, 2, (N, 1))\n",
    "D_in, D_out = x.shape[1], y.shape[1]\n",
    "\n",
    "nn = NeuralNetwork(cost_func=binary_cross_entropy, learning_rate=1e-3)\n",
    "nn.layers.append(Layer(input_dim=D_in, output_dim=4, activation=relu))\n",
    "nn.layers.append(Layer(input_dim=4, output_dim=1, activation=tanh))\n",
    "nn.layers.append(Layer(input_dim=1, output_dim=2, activation=sigmoid))\n",
    "nn.layers.append(Layer(input_dim=2, output_dim=5, activation=leaky_relu))\n",
    "nn.layers.append(Layer(input_dim=5, output_dim=3, activation=elu))\n",
    "nn.layers.append(Layer(input_dim=3, output_dim=D_out, activation=sigmoid))\n",
    "\n",
    "nn.fit(x, y, epochs=100)\n",
    "gradient_checking(nn, x, y, eps=1e-4, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação Multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/100 loss_train: 0.12266334\n",
      "epoch:   10/100 loss_train: 0.12896237\n",
      "epoch:   20/100 loss_train: 0.13561368\n",
      "epoch:   30/100 loss_train: 0.14264357\n",
      "epoch:   40/100 loss_train: 0.15003052\n",
      "epoch:   50/100 loss_train: 0.15641980\n",
      "epoch:   60/100 loss_train: 0.16203421\n",
      "epoch:   70/100 loss_train: 0.16764310\n",
      "epoch:   80/100 loss_train: 0.17338276\n",
      "epoch:   90/100 loss_train: 0.17926592\n",
      "epoch:  100/100 loss_train: 0.18529324\n",
      "epoch:    0/0 loss_train: 1.09074880\n",
      "\u001b[92mGRADIENTS OK\n",
      "Relative error: 1.466199242106662e-09\n"
     ]
    }
   ],
   "source": [
    "y = np.random.randint(0, 2, (N, 1))\n",
    "y_oh = OneHotEncoder(sparse=False, categories='auto').fit_transform(y)\n",
    "D_in, D_out = x.shape[1], y_oh.shape[1]\n",
    "\n",
    "nn = NeuralNetwork(cost_func=softmax_neg_log_likelihood, learning_rate=1e-3)\n",
    "nn.layers.append(Layer(input_dim=D_in, output_dim=4, activation=relu))\n",
    "nn.layers.append(Layer(input_dim=4, output_dim=1, activation=tanh))\n",
    "nn.layers.append(Layer(input_dim=1, output_dim=2, activation=sigmoid))\n",
    "nn.layers.append(Layer(input_dim=2, output_dim=5, activation=leaky_relu))\n",
    "nn.layers.append(Layer(input_dim=5, output_dim=3, activation=elu))\n",
    "nn.layers.append(Layer(input_dim=3, output_dim=D_out, activation=linear))\n",
    "\n",
    "nn.fit(x, y, epochs=100)\n",
    "gradient_checking(nn, x, y_oh, eps=1e-4, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Referências "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Notebook completo sobre Redes Neurais](https://github.com/arnaldog12/Manual-Pratico-Deep-Learning/blob/master/Rede%20Neural.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
